{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detail can be found in my report and litReview file on this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First off, it was nice to see ResNet in action making changes in distribution of output and gradient values. Below output values of an arbitrary layer are shown on the left throughout the iterations (dimension into the screen). Corresponding gradient values are shown to the right. Without ResNet connections we are basically dealing with a large VGGNet where such deep network results in a bumpy spiky loss landscape therefore those activation values are vascillating so much. Introduction of a ResNet connection alleviates the situation. Gradient values stand to the fact that ResNet follows a smoother stable convergence. Those large values for NonResnet gradients are because of the spiky surface ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>With ResNet connection</td></tr>\n",
    "    <tr><td>Activations</td><td>Gradient values</td></tr>\n",
    "    <tr><td><img src='.dbfh/Resact.png' style='height: 300px;'></td><td><img src='.dbfh/Resbact.png' style='height: 300px;'></td></tr>\n",
    "    <tr><td>Without ResNet connection</td></tr>\n",
    "    <tr><td>Activations</td><td>Gradient values</td></tr>\n",
    "    <tr><td><img src='.dbfh/NonResact.png' style='height: 300px;'></td><td><img src='.dbfh/NonResbact.png' style='height: 300px;'></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initialization is sooooo important. Take a look at the evolution of activation distribution below:\n",
    "- Normal initialization starts out normal but literally blows up the activations by the by the 48th layer. Activation values are in the neighborhood of 200s.\n",
    "- Kaiming improves and most activations are kept around (-10, 10)\n",
    "- SUV however is fantastic compared to the other too. Very little variations throughout the layers and even after 47 layers the maximum values are around 20s and there are not a lot of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "   <tr><td><img src='.dbfh/conv1norm.png'></td><td><img src='.dbfh/layer2norm.png'></td><td><img src='.dbfh/layer4norm.png'></td></tr>\n",
    "   <tr><td><img src='.dbfh/conv1xav.png'></td><td><img src='.dbfh/layer2xav.png'></td><td><img src='.dbfh/layer4xav.png'></td></tr>\n",
    "    <tr><td><img src='.dbfh/conv1suv.png'></td><td><img src='.dbfh/layer2suv.png' style=\"height: 280px\"></td><td><img src='.dbfh/layer4suv.png'></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Batch normalization is used to fend off covariate shift to some extent and lets watch the percentiles of an arbitrary layer:\n",
    "- Using Batch normalization results in caging the activations mostly in a fixed boundary throughout the training steps whereas no batch normalization results in asymetric out of shape distribution fanning out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "   <tr><td><img src='.dbfh/Conv1Percentiles-BatchNorm.png'></td><td><img src='.dbfh/Conv1Percentiles.png'></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also was interested to see which kind of augmentation works best for test time on this data set:\n",
    "- Apparently horizontal flip works well and has been a better option in the literature. \n",
    "### more detail can be found in my report file on this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='.dbfh/mean.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
